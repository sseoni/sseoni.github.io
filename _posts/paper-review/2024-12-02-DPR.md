---
layout: post

title: "Dense Passage Retrieval for Open-Domain Question Answering"
date: 2024-12-02
tags: [논문리뷰, LLM, DPR]
summary:
---

### Open-Domain Question Answering

위키피디아 문서를 retrieve해서 ODQA 해결

### Learnable retriever

TF-IDF 간의 BM25라는 지표를 사용해서 검색

TF-IDF 기반 검색 ⇒ Sparse한 retrieval

: Sparse한 retrieval (질문과 응답문장 간의 키워드 자체가 동일한 경우에 잘 동작한다)

### DPR : Dense Passage Retrieval

Vector로 변환해서 검색 ⇒ Dense vector retrieval

: 질문과 응답 문장 간의 키워드가 bad guy ↔ villain 으로 달라도 잘 동작한다

BERT의 pre-trained된 embedding을 활용하는 기법

### ICT pre-training

retriever를 학습시키는 과정에서 ICT (Inverse Cloze Task)라는 사전학습 수행

여러 문단 중 하나의 문단에서 임의로 문장을 빼내고 그 문장과 어떤 문단이 가장 유사도가 높은지를 계산해서 유사도를 구함

[단점]
1. 연산에 부담
2. 일반 문장이 질문을 대용할 수 있는지는 not completely clear
passage에서 뽑은 문장을 pseudo question이라고 하지만 질문이 아니라 일반 문장인데 question의 형태를 완전히 커버할 수 있는지에 대해 의문
3. passage encoder는 freeze하고, question encoder와 reader model만 fine-tuning을 통해서 업데이트 하는데 context에 대한 임베딩이 update되지 않기 때문에 QA task에 있어 optimal한 solution이 아니다

**→ DPR : 추가적인 pre-train없이 question + passage만 갖고 dense embedding model을 학습시킬 수 있는가에 집중**

⇒ open-domain QA task에 있어 reader보다는 retrieval의 성능 향상을 도모

# DPR

목적 : M개의 passage가 주어졌을 때 모든 passage에 대한 Index를 연속적인 저차원 공간에 매핑하는 것

(이 때 passage의 개수 M은 매우 크다)

그 안에서 question과 연관된 top-k개의 passage를 빠르게 효과적으로 추출
k = 20-100

Question에 대한 BERT embedding과 Passage에 대한 BERT embedding으로 이뤄짐

임베딩된 두 값을 비교해서 유사한 passage를 빠르게 추출

**[Inference]**
모델의 파라미터가 고정되어있기 때문에 벡터 간 similarity를 빠르게 계산할 수 있는 Faiss라이브러리를 사용해서 embedding vector를 미리 계산하고, 새로운 질문이 입력으로 들어왔을 때 질문과 연관된 passage를 빠르게 반환한다

**[Training]**

목적 : 연관된 question-passage끼리 가까운 공간에 위치하는 vector space 구축

![사진](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/3f9df703-7a92-4df1-88a9-f7ae72a7a60d/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-03_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.21.30.png)

하나의 question에 대해 negative example은 너무 많음(하나의 passage 제외한 대부분의 passage가 negative)
→ negative example에 대한 신중한 선택이 뛰어난 encoder를 만들 수 있음

Negative example 선정하는 방법
1. Random : corpus에서 아무 passage 선택
2. BM25 : 정답을 포함하지 않지만 정답인 token이 가장 많이 포함되는 passage
3. Gold : 다른 question의 정답을 이 question의 negative example로 사용

### in-batch negatives

하나의 question에 대해 할당된 negative example 이외에 다른 mini batch에 있는 정보도 negative sample로 활용할 수 있다

5개의 mini-batch로 구성된 question embedding
각 embedding의 size는 3
→ 5x3 metric Q

passage의 embedding도 3
5개의 각 질문과 대응하는 5개의 positive passage
→ 3x5 metric P^T

두 metric을 곱하면 5x5 metric S

metric S에서 색칠된 부분 = positive passage의 유사도

색칠 안된부분 = negative passage의 유사도

![사진](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/0e6d63ba-7d4b-4f14-9169-c21a368ae8d8/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-03_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.09.52.png)

dpr에서 유사도 계산할 때 question은 출처가 어디인가?