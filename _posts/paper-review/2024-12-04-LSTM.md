---
layout: post
permalink: /blog/paper-review/24120401

title: "Long Short-Term Memory"
date: 2024-12-04
tags: [논문리뷰, LLM, LSTM]
summary:
---

### Time-Series Data 시계열 데이터

일정 시간동안 수집된 순차적 데이터

시간 순서에 따라 feature들이 서로 상관관계를 가짐

동영상, 주가, 공정 센서 측정 데이터 등

시계열 데이터를 다루는 방법 top2

1. Data Approach
    
    과거에는 SVM, Random Forest등 활용하는 경우가 많았음
    
    DTW(Dynamic Time Warping), FFT, Time-Lag Feature(여러 시퀀스들의 데이터를 Lag로 추가하는 방법)
    
2. Model Approach
    
    확률통계적 방법 : HMM(Hidden Markov Model), ARIMA : 과거에 주로 사용
    
    Deep Learning Model
    : Vanilla RNN, LSTM, GRU, 1D-CNN, Transformer, TCN, Neural ODE, LTFS-Linear
    
    ### Vanilla RNN
    
    fully connected network와 유사 : 입력값 xₜ가 유닛에 들어오고 hₜ가 출력됨
    
    FCN과 차이점 : step의 output이 다음 step의 input = 순환신경망
    
    ![스크린샷 2024-12-04 오후 3.41.55.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/fa67478b-6ecf-4d57-8060-f11494ff168b/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-04_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.41.55.png)
    
    모든 step의 출력을 사용하거나 마지막 step의 output만 사용할 수 있음
    
    ![스크린샷 2024-12-05 오후 2.34.06.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/09c627ef-b643-456e-a9f4-d671754f1930/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.34.06.png)
    
    기존 RNN의 한계점 : BPTT활용으로 인해 Vanishing Gradient, Exploding Gradient발생 → 10이상의 long time step에 대해서 학습이 이루어지지 않음
    
    1. 긴 time dependency를 갖는 경우 Back-propagation할 때 동일한 행렬곱(Wₕ)의 반복으로 인해 1보다 작은 Wₕ값을 반복적으로 곱하는 경우 Gradient소실, 1보다 큰 값을 곱하는 경우 Exploding
    2. Activation function인 tanh를 미분하면 1보다 작은 값이 나오기 때문에 long sequence에서 gradient를 전달할 때 소실됨

## Long Short-Term Memory

하나의 unit에서 계산된 정보(short-term)를 다음 time step으로 길게 전파하는 memory 구조

### contribution & Methodology

- 장기기억 cell인 CEC(Constant Error Carousel)을 도입하여 Vanishing Gradient를 막음
    
    cell state라는 새로운 정보 전달 라인
    
    **CEC**(Constant Error Carousel) : 장기기억 cell, LSTM의 핵심요소, 기울기 손실 막는데 가장 큰 기여를 하는 부분
    
    <aside>
    
    이전 스탭의 Cₜ₋₁을 더한 값을 다음 step의 Cₜ값으로 전달
    : 단순한 플러스 연산이기 때문에 gradient 정보손실 없이 back propagation 가능 → Vanishing Gradient문제 해결
    
    = Cₜ₋₁에 weight 1.0 곱한 것
    
    = Cₜ₋₁에서 Cₜ로의 identity mapping 혹은 identity function
    
    → BPTT로 인한 Vanishing Gradient문제 해결
    
    → 더 깊은 network 학습 = 더 긴 time 학습
    
    </aside>
    
    ** Cₜ₋₁값을 그대로 Cₜ로 전달하면 아무런 정보변화가 없기 때문에 게이트에서 정보에 대한 업데이트
    
- Gate를 도입하여 학습 기반으로 input/output값에 대한 자동제어
(original LSTM에는 Forget Gate가 없음)
    
    장기적으로 유지되어야할 cell state정보가 자동으로 업데이트되어 출력됨
    → 장기기억 및 예측 성능향상
    
    1. input gate
        
        $$
        i_t = σ(W_i * [h_{t-1}, x_t] + bias)
        $$
        
        $$
        S_t = squash(W_c * [h_{t-1}, x_t] + bias)
        $$
        
        ![스크린샷 2024-12-05 오전 1.58.29.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/e784a849-4e3a-4b3f-bf41-2fc726f38934/05ae0ab8-5f7b-4afc-9cab-8660d1118316.png)
        
        hₜ₋₁ : 이전 유닛의 output
        xₜ : 현재 step의 input
        Sₜ : hₜ₋₁과 xₜ 두 input 정보 계산
        
        sigmoid function으로 input이 얼마나 gate에 들어가야할지 결정
        
    2. output gate
        
        LSTM에서 유지되고있는 sell state정보를 output인 hₜ값 제어
        
        sigmoid function으로 hₜ₋₁과 xₜ을 통해 출력해야할 비율 결정
        
        squash로 activation된 cell 값에 조절을 한 후 output hₜ
        
        $$
        O_t = σ(W_i * [h_{t-1}, x_t] + bias)
        $$
        
        $$
        h_t = O_t + squash(C_t)
        $$
        
        ![스크린샷 2024-12-05 오전 3.19.29.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/7085c00b-0cf9-454d-b61c-507bd768e79c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-05_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_3.19.29.png)
        
- BPTT과 RTRL의 variation을 동시에 학습해 학습 효율을 향상
    
    **BPTT:Back Propagation Through Time**
    
    속도가 빠르며 O(n^2) 일반적으로 많이 사용
    
    Backward 연산이 따로 필요하고 Vanishing Gradient가 잘 발생
    
    **RTRL:Real-Time Recurrent Learning**
    
    속도는 느리지만 O(n^4)
    
    BPTT보다는 vanishing gradient에 강함
    
    메모리 사용량이 적고 실시간 학습에 적합
     : forword할 때 미분값 계산하기 때문에 backward연산이 없어 메모리 사용량이 낮음
    
    ![스크린샷 2024-12-05 오전 3.34.43.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/7b244b43-0500-471b-9096-37e4d91d42cb/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-05_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_3.34.43.png)
    
    보통 하나만 단독사용하는데 이 논문에서는 input value, cell state, input gate에서는 BPTT, output gate, value에서는 RTRL을 사용
    

** 왜 squash function으로 hyperbolic function의 변형을 사용하는가?

sigmoid function은 미분값이 최대 0.25 → 역전파 시 vanishing 심해짐

하지만 hyperbolic tanh은 미분값이 0에서 1 사이 → 게이트와 함께 사용 시 vanishing, explorer 줄일 수 있음

## Current Vanilla LSTM

![스크린샷 2024-12-05 오후 2.28.33.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/ee9d9242-e3ea-4346-a208-8c33837aabc1/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-05_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.28.33.png)

학습 알고리즘 : Fully BPTT

현재 구조는 약 1,000 step 까지 학습 가능