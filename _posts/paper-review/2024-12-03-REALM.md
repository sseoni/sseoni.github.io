---
layout: post
permalink: /blog/paper-review/24120301

title: "REALM: Retrieval-Augmented Language Model Pre-Training"
date: 2024-12-03
tags: [논문리뷰, LLM, REALM]
summary:
---

### Two stage : Retriever-Reader approach
 = question - answering 두개로 decompose

비지도 학습 기반으로 유사도가 높은 문서를 찾는 retrieval 진행 → reader model에서 학습을 진행해서 answer를 찾아냄

## REALM

retriever 자체를 end2end training

BERT의 저자가 REALM의 제2저자 → pre-train방법을 통해 retriever과 reader를 통합하는 end2end framework

1. retriever도 학습하면 QA에 성능이 높아질 수 있다
2. retriever와 reader를 한번에 학습하는 joint training
3. retriever를 pre-training에서 수행하는 모델 제안

### Main contribution

1. retriever와 reader를 한번에 학습하는 E2E모델
2. query를 넣어서 답을 찾는 과정을 두 단계로 분리
    
    neural knowledge retriever : 쿼리에서 답이 될만한 document를 찾음
    
    knowledge-augmented encoder : retrieved document를 통해 answer를 찾아내는 과정
    
3. pre-training과 fine-tuning을 모두 진행

![사진](https://prod-files-secure.s3.us-west-2.amazonaws.com/5f9be9c0-fb3c-4288-8b16-c29b6ff83dd2/e7193258-4b30-4dfe-aaa4-401330ab7eed/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-12-03_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.28.03.png)

**unlabeled text를 통해 pre-train**

: pre-train = 기존의 mask language modeling을 활용하는데 이걸 활용해서 모델을 학습하겠다가 아니라 mask language modeling 자체가 QA에 도움이 될 수 있도록

text는 unlabeled + mask token도 붙어있지 않음